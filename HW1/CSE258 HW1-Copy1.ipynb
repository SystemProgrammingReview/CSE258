{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE258 HW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import urllib\n",
    "import scipy.optimize\n",
    "import random\n",
    "import csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import svm\n",
    "from math import exp\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "def parseData(fname):\n",
    "    for l in urllib.urlopen(fname):\n",
    "        yield eval(l)\n",
    "\n",
    "print \"Reading data...\"\n",
    "data = list(parseData(\"http://jmcauley.ucsd.edu/cse255/data/beer/beer_50000.json\"))\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x1 = [[1,i['review/timeStruct']['year']] for i in data]\n",
    "\n",
    "y = [i['review/overall'] for i in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We first train a predictor with linear regression as below:\n",
    "\n",
    "#### $'review/overall' = \\theta_0 + \\theta_1 * 'year'$\n",
    "\n",
    "#### The fitted values are $\\theta_0 = -3.917e+01, \\theta_1 = 2.1438e-02$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -3.91707489e+01   2.14379786e-02]\n",
      "[ 0.49004382]\n"
     ]
    }
   ],
   "source": [
    "result1 = np.linalg.lstsq(x1,y)\n",
    "print result1[0]\n",
    "MSE1 = result1[1]/50000\n",
    "print MSE1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Q2\n",
    "#### Then we use a second-order polynomial predictor instead of linear one:\n",
    "$'review/overall' = \\theta_0 + \\theta_1 * 'year' + \\theta_2 * 'year'$\n",
    "#### The MSE in Q1 is 0.49004382. While here we have MSE as 0.49003734, which is only little better. Actually, 'review/overall' can depends little on one-dimension feature, 'year', since we see that the beers in the same 'year' may varies a lot in 'review/overall'. So our improvement is little."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x2= []\n",
    "for feat in x1:\n",
    "    newFeat = feat[:]\n",
    "    newFeat.append(feat[1]**2)\n",
    "    x2.append(newFeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -2.32112075e+02   2.13653648e-01  -4.78731119e-05]\n",
      "[ 0.49004374]\n"
     ]
    }
   ],
   "source": [
    "result2 = np.linalg.lstsq(x2,y)\n",
    "print result2[0]\n",
    "MSE2 = result2[1]/50000\n",
    "print MSE2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'quality']\n"
     ]
    }
   ],
   "source": [
    "with open('winequality-white.csv', 'rb') as f:\n",
    "    reader = csv.reader(f, delimiter=';')\n",
    "    wine = []\n",
    "    for row in reader:\n",
    "        wine.append(row)\n",
    "for i in range(1,len(wine)):\n",
    "    for j in range(0,len(wine[i])):\n",
    "        wine[i][j] = float(wine[i][j])\n",
    "print wine[0]\n",
    "wine = wine[1:]\n",
    "train = np.array(wine[0:len(wine)/2])\n",
    "test = np.array(wine[len(wine)/2:len(wine)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The fitted coefficients $\\theta = [\\theta_0, \\theta_1,...]$ & train data MSE (0.6023) is calculated as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x = np.concatenate((np.ones((len(train),1)), train[:,0:-1]), axis=1)\n",
    "train_y = train[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.56420279e+02   1.35421303e-01  -1.72994866e+00   1.02651152e-01\n",
      "   1.09038568e-01  -2.76775146e-01   6.34332169e-03   3.85023977e-05\n",
      "  -2.58652809e+02   1.19540566e+00   8.33006285e-01   9.79304353e-02]\n",
      "[ 0.6023075]\n"
     ]
    }
   ],
   "source": [
    "result3 = np.linalg.lstsq(train_x,train_y)\n",
    "MSE3 = result3[1]/len(train)\n",
    "print result3[0]\n",
    "print MSE3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The MSE on the test data is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x = np.concatenate((np.ones((len(test),1)), test[:,0:-1]), axis=1)\n",
    "test_y = test[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56245713031281874"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(np.dot(test_x, result3[0]), test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Q4\n",
    "#### The MSEs of all 11 ablation experiments are calculated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.55911341437613271, 0.59638485016151266, 0.56222170281155925, 0.55362506396744637, 0.56262926648130007, 0.55614081792995385, 0.56242900546920438, 0.54472655346615018, 0.5595666263819955, 0.55734634998788479, 0.57321474355822255]\n"
     ]
    }
   ],
   "source": [
    "MSE_abl = []\n",
    "for i in range(1,12):\n",
    "    # get ablation feature data\n",
    "    train_abl = np.delete(train_x, i, axis=1)\n",
    "    test_abl = np.delete(test_x, i, axis=1)\n",
    "    # training\n",
    "    fit = np.linalg.lstsq(train_abl,train_y)\n",
    "    # mse\n",
    "    MSE_abl.append(mean_squared_error(np.dot(test_abl, fit[0]), test_y))\n",
    "print MSE_abl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on the test MSEs, we see 'density' provides the least additional information while 'volatile acidity' provides the most."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Q5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Under C=0.8, I have accuracy on train and test data as 89.91% and 69.86%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_lab = map(lambda x : 1 if x>5 else 0, train_y)\n",
    "test_lab = map(lambda x : 1 if x>5 else 0, test_y)\n",
    "clf = svm.SVC(C=0.8)\n",
    "clf.fit(train_x, train_lab)\n",
    "\n",
    "# prediction with classifier\n",
    "train_pred = clf.predict(train_x)\n",
    "test_pred = clf.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pair = np.vstack((np.array(train_lab), train_pred))\n",
    "test_pair = np.vstack((np.array(test_lab), test_pred))\n",
    "train_correct = filter(lambda x : x[0] == x[1], train_pair.T)\n",
    "test_correct = filter(lambda x : x[0] == x[1], test_pair.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.899142507146 0.698652511229\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = len(train_correct) * 1. /len(train_x)\n",
    "test_accuracy = len(test_correct) * 1. /len(test_x)\n",
    "print train_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Q6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The log-likelihood after convergence is -1383.18, and the accuract of the resulting model is 76.68%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inner(x,y):\n",
    "    return sum([x[i]*y[i] for i in range(len(x))])\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1 + exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NEGATIVE Log-likelihood\n",
    "def f(theta, X, y, lam):\n",
    "    loglikelihood = 0\n",
    "    for i in range(len(X)):\n",
    "        logit = inner(X[i], theta)\n",
    "        loglikelihood -= log(1 + exp(-logit))\n",
    "        if not y[i]:\n",
    "            loglikelihood -= logit\n",
    "    for k in range(len(theta)):\n",
    "        loglikelihood -= lam * theta[k]*theta[k]\n",
    "    print \"ll =\", loglikelihood\n",
    "    return -loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NEGATIVE Derivative of log-likelihood\n",
    "def fprime(theta, X, y, lam):\n",
    "    dl = [0.0]*len(theta)\n",
    "    for i in range(len(X)):\n",
    "        xi = sigmoid(inner(X[i], theta))\n",
    "    # Fill in code for the derivative\n",
    "        for j in range(len(dl)):\n",
    "            dl[j] += X[i][j] * (1.0 - xi)\n",
    "            if not y[i]:\n",
    "                dl[j] -= X[i][j]\n",
    "    dl -= lam * 2.0 * theta\n",
    "    # Negate the return value since we're doing gradient *ascent*\n",
    "    return numpy.array([-x for x in dl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ll = -1697.51744519\n",
      "ll = -143197.801827\n",
      "ll = -8508.90496218\n",
      "ll = -1662.30635\n",
      "ll = -1640.40734323\n",
      "ll = -1640.03672607\n",
      "ll = -1639.03710526\n",
      "ll = -1636.45342202\n",
      "ll = -1630.7647578\n",
      "ll = -1620.5379981\n",
      "ll = -1608.46892967\n",
      "ll = -1600.12028955\n",
      "ll = -1597.25108903\n",
      "ll = -1596.43103401\n",
      "ll = -1594.6609878\n",
      "ll = -1590.8669624\n",
      "ll = -1582.36652552\n",
      "ll = -1568.38645219\n",
      "ll = -1573.12710035\n",
      "ll = -1563.12188538\n",
      "ll = -1730.8662104\n",
      "ll = -1553.14766525\n",
      "ll = -1551.33467062\n",
      "ll = -1550.99356131\n",
      "ll = -1550.98432802\n",
      "ll = -1550.94773519\n",
      "ll = -1550.80680828\n",
      "ll = -1550.33015453\n",
      "ll = -1546.68468455\n",
      "ll = -1540.63459947\n",
      "ll = -1535.30077889\n",
      "ll = -1524.47917737\n",
      "ll = -1515.22289358\n",
      "ll = -1500.21666146\n",
      "ll = -1493.66443366\n",
      "ll = -1492.36234834\n",
      "ll = -1492.23495489\n",
      "ll = -1504.29563017\n",
      "ll = -1492.23129908\n",
      "ll = -1492.21829047\n",
      "ll = -1492.11123077\n",
      "ll = -1492.01427129\n",
      "ll = -1491.92385856\n",
      "ll = -1491.49164082\n",
      "ll = -1490.15516329\n",
      "ll = -1487.33551334\n",
      "ll = -1482.64462887\n",
      "ll = -1464.11483057\n",
      "ll = -1568.29935212\n",
      "ll = -1460.46434623\n",
      "ll = -1448.70468341\n",
      "ll = -1439.09750549\n",
      "ll = -1435.20355504\n",
      "ll = -1434.97546587\n",
      "ll = -1441.04320144\n",
      "ll = -1434.85079405\n",
      "ll = -1434.72758573\n",
      "ll = -1434.69618219\n",
      "ll = -1434.69123887\n",
      "ll = -1434.67258334\n",
      "ll = -1434.40531316\n",
      "ll = -1434.04491038\n",
      "ll = -1433.41207169\n",
      "ll = -1432.83108283\n",
      "ll = -1432.42509708\n",
      "ll = -1432.29062007\n",
      "ll = -1432.21408607\n",
      "ll = -1431.90095718\n",
      "ll = -1431.24206781\n",
      "ll = -1429.77010156\n",
      "ll = -1427.39755252\n",
      "ll = -1435.69124297\n",
      "ll = -1426.67526988\n",
      "ll = -1423.16499214\n",
      "ll = -1419.96540955\n",
      "ll = -1407.27220495\n",
      "ll = -1399.1707577\n",
      "ll = -1444.82745518\n",
      "ll = -1398.1340528\n",
      "ll = -1392.68596428\n",
      "ll = -1390.41011674\n",
      "ll = -1388.55340144\n",
      "ll = -1387.00617775\n",
      "ll = -1386.54263354\n",
      "ll = -1386.13042394\n",
      "ll = -1387.57142464\n",
      "ll = -1385.99469518\n",
      "ll = -1385.94021578\n",
      "ll = -1386.31948307\n",
      "ll = -1385.9361226\n",
      "ll = -1385.92535907\n",
      "ll = -1385.92684834\n",
      "ll = -1385.92399326\n",
      "ll = -1385.9193955\n",
      "ll = -1385.9157056\n",
      "ll = -1385.91510551\n",
      "ll = -1385.91505216\n",
      "ll = -1385.91498973\n",
      "ll = -1385.91493041\n",
      "ll = -1385.91459394\n",
      "ll = -1385.91405339\n",
      "ll = -1385.91327845\n",
      "ll = -1386.05711261\n",
      "ll = -1385.91300609\n",
      "ll = -1385.91321515\n",
      "ll = -1385.91275897\n",
      "ll = -1385.91197816\n",
      "ll = -1385.91149273\n",
      "ll = -1385.91120576\n",
      "ll = -1385.91054448\n",
      "ll = -1385.90959022\n",
      "ll = -1385.91028703\n",
      "ll = -1385.90877325\n",
      "ll = -1385.90728339\n",
      "ll = -1385.90513661\n",
      "ll = -1385.90461877\n",
      "ll = -1385.9087722\n",
      "ll = -1385.90154452\n",
      "ll = -1385.91123255\n",
      "ll = -1385.90101486\n",
      "ll = -1385.89833298\n",
      "ll = -1385.89587059\n",
      "ll = -1385.89446179\n",
      "ll = -1385.88770696\n",
      "ll = -1385.87354463\n",
      "ll = -1385.83995978\n",
      "ll = -1385.78390093\n",
      "ll = -1385.74861737\n",
      "ll = -1385.63258359\n",
      "ll = -1385.57781923\n",
      "ll = -1385.5402197\n",
      "ll = -1385.52544421\n",
      "ll = -1385.45672745\n",
      "ll = -1391.23671276\n",
      "ll = -1385.45553771\n",
      "ll = -1385.42179843\n",
      "ll = -1385.39914935\n",
      "ll = -1385.38491187\n",
      "ll = -1385.37018271\n",
      "ll = -1385.33096654\n",
      "ll = -1385.25419878\n",
      "ll = -1385.12271885\n",
      "ll = -1388.96986435\n",
      "ll = -1385.11725397\n",
      "ll = -1384.98557551\n",
      "ll = -1384.91357247\n",
      "ll = -1384.89457114\n",
      "ll = -1384.88880807\n",
      "ll = -1384.88286902\n",
      "ll = -1384.86555723\n",
      "ll = -1384.83514668\n",
      "ll = -1384.79128043\n",
      "ll = -1384.7482923\n",
      "ll = -1384.82467988\n",
      "ll = -1384.73286026\n",
      "ll = -1386.68270369\n",
      "ll = -1384.72927449\n",
      "ll = -1384.71074705\n",
      "ll = -1384.69388735\n",
      "ll = -1384.67743832\n",
      "ll = -1384.64078073\n",
      "ll = -1394.68381372\n",
      "ll = -1384.63603241\n",
      "ll = -1384.59217498\n",
      "ll = -1384.55484858\n",
      "ll = -1384.53273687\n",
      "ll = -1384.50804038\n",
      "ll = -1384.45188213\n",
      "ll = -1384.33219865\n",
      "ll = -1398.26058273\n",
      "ll = -1384.30649539\n",
      "ll = -1384.13021855\n",
      "ll = -1383.9337553\n",
      "ll = -1383.87458455\n",
      "ll = -1383.86922301\n",
      "ll = -1383.86601599\n",
      "ll = -1383.86446458\n",
      "ll = -1383.86412567\n",
      "ll = -1383.86291272\n",
      "ll = -1383.86213262\n",
      "ll = -1383.86243334\n",
      "ll = -1383.86190038\n",
      "ll = -1384.56248383\n",
      "ll = -1383.86186254\n",
      "ll = -1383.86159776\n",
      "ll = -1383.86153972\n",
      "ll = -1383.86153043\n",
      "ll = -1383.86150105\n",
      "ll = -1383.86140583\n",
      "ll = -1383.8609048\n",
      "ll = -1383.9525305\n",
      "ll = -1383.86076189\n",
      "ll = -1383.85910832\n",
      "ll = -1383.85646453\n",
      "ll = -1383.85481059\n",
      "ll = -1383.84855414\n",
      "ll = -1383.84657862\n",
      "ll = -1383.84564225\n",
      "ll = -1383.84541503\n",
      "ll = -1383.84813748\n",
      "ll = -1383.84539203\n",
      "ll = -1383.84524392\n",
      "ll = -1383.84512003\n",
      "ll = -1383.84497468\n",
      "ll = -1383.84494037\n",
      "ll = -1383.84487456\n",
      "ll = -1383.84480839\n",
      "ll = -1383.84472488\n",
      "ll = -1383.84452943\n",
      "ll = -1383.84444509\n",
      "ll = -1383.84432108\n",
      "ll = -1383.84427073\n",
      "ll = -1383.84423205\n",
      "ll = -1383.8447789\n",
      "ll = -1383.84417693\n",
      "ll = -1383.84405435\n",
      "ll = -1383.84379189\n",
      "ll = -1383.84363639\n",
      "ll = -1383.84351834\n",
      "ll = -1383.84414698\n",
      "ll = -1383.84342816\n",
      "ll = -1383.84402972\n",
      "ll = -1383.84340042\n",
      "ll = -1383.84321631\n",
      "ll = -1383.84298297\n",
      "ll = -1383.84272471\n",
      "ll = -1383.84216551\n",
      "ll = -1383.84148533\n",
      "ll = -1383.94283079\n",
      "ll = -1383.84144052\n",
      "ll = -1383.84065486\n",
      "ll = -1383.84014554\n",
      "ll = -1383.84003824\n",
      "ll = -1383.84001643\n",
      "ll = -1383.83997652\n",
      "ll = -1383.83982176\n",
      "ll = -1383.83950089\n",
      "ll = -1384.01104341\n",
      "ll = -1383.83939222\n",
      "ll = -1383.83881762\n",
      "ll = -1383.8376224\n",
      "ll = -1385.55321926\n",
      "ll = -1383.83760804\n",
      "ll = -1383.83720367\n",
      "ll = -1383.8370521\n",
      "ll = -1383.83701003\n",
      "ll = -1383.83678951\n",
      "ll = -1383.8364412\n",
      "ll = -1383.83568051\n",
      "ll = -1383.89722084\n",
      "ll = -1383.83534972\n",
      "ll = -1383.834363\n",
      "ll = -1383.83291001\n",
      "ll = -1383.83261383\n",
      "ll = -1383.83242728\n",
      "ll = -1383.8321944\n",
      "ll = -1383.93027884\n",
      "ll = -1383.83197077\n",
      "ll = -1383.83114805\n",
      "ll = -1383.82935895\n",
      "ll = -1383.82136492\n",
      "ll = -1383.81192986\n",
      "ll = -1383.82310227\n",
      "ll = -1383.809035\n",
      "ll = -1383.80062331\n",
      "ll = -1383.79836995\n",
      "ll = -1383.7973495\n",
      "ll = -1383.79649829\n",
      "ll = -1383.79457831\n",
      "ll = -1383.79071295\n",
      "ll = -1383.78750546\n",
      "ll = -1383.78541751\n",
      "ll = -1383.7848567\n",
      "ll = -1383.78180737\n",
      "ll = -1396.13772635\n",
      "ll = -1383.78138977\n",
      "ll = -1383.77878385\n",
      "ll = -1383.78974973\n",
      "ll = -1383.77674871\n",
      "ll = -1383.76936179\n",
      "ll = -1485.37174783\n",
      "ll = -1383.76884798\n",
      "ll = -1383.76011875\n",
      "ll = -1383.75586928\n",
      "ll = -1388.30581563\n",
      "ll = -1383.75518252\n",
      "ll = -1383.75321878\n",
      "ll = -1384.18846203\n",
      "ll = -1383.75267118\n",
      "ll = -1383.75069533\n",
      "ll = -1384.09842368\n",
      "ll = -1383.74985466\n",
      "ll = -1383.74408657\n",
      "ll = -1383.83761576\n",
      "ll = -1383.74217846\n",
      "ll = -1383.73647875\n",
      "ll = -1386.07135567\n",
      "ll = -1383.73633756\n",
      "ll = -1383.73146338\n",
      "ll = -1383.69629234\n",
      "ll = -1393.91928113\n",
      "ll = -1383.69595643\n",
      "ll = -1383.66364323\n",
      "ll = -1388.76160086\n",
      "ll = -1383.6628899\n",
      "ll = -1383.66133286\n",
      "ll = -1383.67637552\n",
      "ll = -1383.65413809\n",
      "ll = -1383.63610778\n",
      "ll = -1384.86245459\n",
      "ll = -1383.62620622\n",
      "ll = -1383.57413389\n",
      "ll = -1383.46856499\n",
      "ll = -1383.30891171\n",
      "ll = -1383.21873657\n",
      "ll = -1383.20615968\n",
      "ll = -1383.19501975\n",
      "ll = -1383.19353492\n",
      "ll = -1383.19215088\n",
      "ll = -1392.45642952\n",
      "ll = -1383.19193978\n",
      "ll = -1383.19148305\n",
      "ll = -1383.19097028\n",
      "ll = -1383.19055615\n",
      "ll = -1383.18988757\n",
      "ll = -1383.18897906\n",
      "ll = -1383.18882095\n",
      "ll = -1383.18792012\n",
      "ll = -1383.18762333\n",
      "ll = -1383.18728975\n",
      "ll = -1383.23335757\n",
      "ll = -1383.18728431\n",
      "ll = -1383.18719142\n",
      "ll = -1383.18703379\n",
      "ll = -1383.18690509\n",
      "ll = -1383.18678271\n",
      "ll = -1383.21349316\n",
      "ll = -1383.18676796\n",
      "ll = -1383.18678101\n",
      "ll = -1383.18672465\n",
      "ll = -1383.18668652\n",
      "ll = -1383.18680532\n",
      "ll = -1383.18665615\n",
      "ll = -1383.18655005\n",
      "ll = -1383.18638744\n",
      "ll = -1383.18612636\n",
      "ll = -1383.18591542\n",
      "ll = -1383.18581585\n",
      "ll = -1383.18572666\n",
      "ll = -1383.18571527\n",
      "ll = -1383.18571158\n",
      "ll = -1383.18567597\n",
      "ll = -1383.18569354\n",
      "ll = -1383.18565575\n",
      "ll = -1383.18569544\n",
      "ll = -1383.18563628\n",
      "ll = -1383.18561493\n",
      "ll = -1383.18576273\n",
      "ll = -1383.18554798\n",
      "ll = -1383.18548968\n",
      "ll = -1383.18555303\n",
      "ll = -1383.18547457\n",
      "ll = -1383.18544541\n",
      "ll = -1383.18543481\n",
      "ll = -1383.18543136\n",
      "ll = -1383.18543063\n",
      "Final log likelihood = -1383.18543063\n",
      "Accuracy =  0.766843609637\n"
     ]
    }
   ],
   "source": [
    "# If we wanted to split with a validation set:\n",
    "#X_valid = X[len(X)/2:3*len(X)/4]\n",
    "#X_test = X[3*len(X)/4:]\n",
    "\n",
    "# Use a library function to run gradient descent (or you can implement yourself!)\n",
    "theta,l,info = scipy.optimize.fmin_l_bfgs_b(f, [0]*len(train_x[0]), fprime, args = (train_x, train_lab, 1.0))\n",
    "print \"Final log likelihood =\", -l\n",
    "# predict the test data\n",
    "test_pred = map(lambda x: 0 if x<0.5 else 1, [sigmoid(inner(X, theta)) for X in test_x])\n",
    "test_pair = np.vstack((np.array(test_lab), test_pred))\n",
    "test_correct = filter(lambda x: x[0] == x[1], test_pair.T)\n",
    "print \"Accuracy = \", len(test_correct) * 1.0 / len(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
